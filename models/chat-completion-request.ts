/*
Earth-2 Inference Server

API for submitting to the Earth-2 Inference Server.

The version of the OpenAPI document: 0.1.0
Contact: earth2-support@exchange.nvidia.com

NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/
import type * as buffer from "buffer"

import { MessagesProperty } from './messages-property';
import { Null } from './null';
import { StopProperty } from './stop-property';

/**
 * OpenAI ChatCompletionRequest
 * @export
 * @interface ChatCompletionRequest
 */
export interface ChatCompletionRequest {
    /**
     * 
     * @type {string}
     * @memberof ChatCompletionRequest
     */
    'model'?: string;
    /**
     * The maximum number of tokens to generate in any given call. Note that the model is not aware of this value, and generation will simply stop at the number of tokens specified.
     * @type {number}
     * @memberof ChatCompletionRequest
     */
    'max_tokens'?: number;
    /**
     * If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events (SSE) as they become available (JSON responses are prefixed by `data: `), with the stream terminated by a `data: [DONE]` message.
     * @type {boolean}
     * @memberof ChatCompletionRequest
     */
    'stream'?: boolean;
    /**
     * The sampling temperature to use for text generation. The higher the temperature value is, the less deterministic the output text will be. It is not recommended to modify both temperature and top_p in the same call.
     * @type {number}
     * @memberof ChatCompletionRequest
     */
    'temperature'?: number;
    /**
     * The top-p sampling mass used for text generation. The top-p value determines the probability mass that is sampled at sampling time. For example, if top_p = 0.2, only the most likely tokens (summing to 0.2 cumulative probability) will be sampled. It is not recommended to modify both temperature and top_p in the same call.
     * @type {number}
     * @memberof ChatCompletionRequest
     */
    'top_p'?: number;
    /**
     * 
     * @type {StopProperty}
     * @memberof ChatCompletionRequest
     */
    'stop'?: StopProperty | null;
    /**
     * Indicates how much to penalize new tokens based on their existing frequency in the text so far, decreasing model likelihood to repeat the same line verbatim.
     * @type {number}
     * @memberof ChatCompletionRequest
     */
    'frequency_penalty'?: number;
    /**
     * Positive values penalize new tokens based on whether they appear in the text so far, increasing model likelihood to talk about new topics.
     * @type {number}
     * @memberof ChatCompletionRequest
     */
    'presence_penalty'?: number;
    /**
     * The model generates random results. Changing the input seed alone will produce a different response with similar characteristics. It is possible to reproduce results by fixing the input seed (assuming all other hyperparameters are also fixed).
     * @type {Null}
     * @memberof ChatCompletionRequest
     */
    'seed'?: Null;
    /**
     * 
     * @type {MessagesProperty}
     * @memberof ChatCompletionRequest
     */
    'messages': MessagesProperty;
}

