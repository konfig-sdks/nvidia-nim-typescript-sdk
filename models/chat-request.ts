/*
Earth-2 Inference Server

API for submitting to the Earth-2 Inference Server.

The version of the OpenAPI document: 0.1.0
Contact: earth2-support@exchange.nvidia.com

NOTE: This file is auto generated by Konfig (https://konfigthis.com).
*/
import type * as buffer from "buffer"

import { MessagesProperty1Inner } from './messages-property1-inner';

/**
 * 
 * @export
 * @interface ChatRequest
 */
export interface ChatRequest {
    /**
     * A list of messages comprising the conversation so far. The roles of the messages must be alternating between `user` and `assistant`. The last input message should have role `user` or `assistant`. A message with the `system` role is optional, and must be the very first message if it is present.
     * @type {Array<MessagesProperty1Inner>}
     * @memberof ChatRequest
     */
    'messages': Array<MessagesProperty1Inner>;
    /**
     * The sampling temperature to use for text generation. The higher the temperature value is, the less deterministic the output text will be. It is not recommended to modify both temperature and top_p in the same call.
     * @type {number}
     * @memberof ChatRequest
     */
    'temperature'?: number;
    /**
     * The top-p sampling mass used for text generation. The top-p value determines the probability mass that is sampled at sampling time. For example, if top_p = 0.2, only the most likely tokens (summing to 0.2 cumulative probability) will be sampled. It is not recommended to modify both temperature and top_p in the same call.
     * @type {number}
     * @memberof ChatRequest
     */
    'top_p'?: number;
    /**
     * The maximum number of tokens to generate in any given call. Note that the model is not aware of this value, and generation will simply stop at the number of tokens specified.
     * @type {number}
     * @memberof ChatRequest
     */
    'max_tokens'?: number;
    /**
     * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
     * @type {number}
     * @memberof ChatRequest
     */
    'seed'?: number | null;
    /**
     * If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events (SSE) as they become available (JSON responses are prefixed by `data: `), with the stream terminated by a `data: [DONE]` message.
     * @type {boolean}
     * @memberof ChatRequest
     */
    'stream'?: boolean;
    /**
     * Grounding refers to the process of connecting or associating text with the visual world by  <br>perceiving object descriptions, such as bounding boxes, within an image. So if set, the model will produce  <br>bounding boxes for entities mentioned in the generated text.
     * @type {boolean}
     * @memberof ChatRequest
     */
    'grounded_response'?: boolean;
    /**
     * [Deprecated parameter]: use `grounded_response` instead
     * @type {boolean}
     * @memberof ChatRequest
     * @deprecated
     */
    'bounding_boxes'?: boolean | null;
    /**
     * These modes determine the level of information provided in the response.  <br>When it is set `detailed` response will have a bit more information
     * @type {string}
     * @memberof ChatRequest
     */
    'response_mode'?: ChatRequestResponseModeEnum;
    /**
     * Model is capable of performing two tasks: Visual Question Answering (VQA) and image captioning.  <br>Visual Question Answering involves answering questions about an image. Image captioning, on the other hand,  <br>generates descriptive captions for images. In image captioning both content and entities are ignored
     * @type {string}
     * @memberof ChatRequest
     */
    'task'?: ChatRequestTaskEnum;
}

type ChatRequestResponseModeEnum = 'brief' | 'detailed'
type ChatRequestTaskEnum = 'VQA' | 'image captioning'


